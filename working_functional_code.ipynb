{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d17e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import skimage\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tifffile\n",
    "import numpy as np\n",
    "from PIL import TiffImagePlugin\n",
    "import zarr as z\n",
    "import tifffile as tiff\n",
    "\n",
    "import apoc\n",
    "from skimage.io import imread\n",
    "import pyclesperanto_prototype as cle\n",
    "import matplotlib.pyplot as plt\n",
    "from apoc import PixelClassifier\n",
    "from pyclesperanto_prototype import imshow\n",
    "import bioformats2raw\n",
    "from napari_skimage_regionprops import regionprops_table, add_table, get_table\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "#import pyvista\n",
    "import argparse\n",
    "import itk\n",
    "from distutils.version import StrictVersion as VS\n",
    "\n",
    "import kimimaro\n",
    "from skimage import measure\n",
    "from skimage import filters\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import math\n",
    "\n",
    "from gurobipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bmp to tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/data_1/\"\n",
    "os.remove(\"/Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/data_1/.DS_Store\")\n",
    "sorted(os.listdir(input_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d731e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/data_rawTiff\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in sorted(os.listdir(input_directory)):\n",
    "    filepath = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/data_1/' + filename\n",
    "    if filename.endswith(\".bmp\"):\n",
    "        img = Image.open(filepath)\n",
    "     #tiff.imwrite(file_name, img, bigtiff=True, photometric='rgb')\n",
    "        img.save('/Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/data_rawTiff/' + filename.replace('.bmp' , '.tiff'), format='TIFF', compression='tiff_lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fee62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#view it if needed\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tiff sequence to multipage tiff using FIJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952393c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert multipage tiff to compressed format zarr: just for visulization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d750389",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf2raw_dir = '/Users/ramyarajalakshmi/Documents/bioformats2raw-0.2.0/'\n",
    "\n",
    "bf2raw=os.path.join(bf2raw_dir, \"bioformats2raw\")\n",
    "\n",
    "tifffile = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/26053_eye1/26053_eye1_raw.tif\"\n",
    "\n",
    "os.chdir(bf2raw_dir)\n",
    "\n",
    "cmd='bioformats2raw /Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/26053_eye1/26053_eye1_raw.tif /Users/ramyarajalakshmi/Documents/Segmentation/mv_eyes/method_1/26053_eye1/26053_eye1_raw.zarr'\n",
    "\n",
    "print('Command String : ', cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6707804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view it if needed\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d309f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train random forest - requirements: vesselness filter, manual labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vesselness filter for the data - use multipage tiff as input to the below filter obtained from simpleITK- Multiscale hessian based vesselness filter\n",
    "#run this script in workstation, in laptop the kernel will die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe480a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TIFFReadDirectory: Warning, Unknown field with tag 50838 (0xc696) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50839 (0xc697) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50838 (0xc696) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50839 (0xc697) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50838 (0xc696) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50839 (0xc697) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50838 (0xc696) encountered.\n",
      "TIFFReadDirectory: Warning, Unknown field with tag 50839 (0xc697) encountered.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import itk\n",
    "from distutils.version import StrictVersion as VS\n",
    "\n",
    "if VS(itk.Version.GetITKVersion()) < VS(\"5.0.0\"):\n",
    "    print(\"ITK 5.0.0 or newer is required.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "    \n",
    "input_image = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/kristin_data/1650/1650_rawTiff.tif\"\n",
    "output_image = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/kristin_data/1650/1650_rawTiff_vesselness.tif\"\n",
    "sigma_minimum = 1.0\n",
    "sigma_maximum = 10.0\n",
    "number_of_sigma_steps = 10\n",
    "\n",
    "\n",
    "input_image = itk.imread(input_image, itk.F)\n",
    "\n",
    "ImageType = type(input_image)\n",
    "Dimension = input_image.GetImageDimension()\n",
    "HessianPixelType = itk.SymmetricSecondRankTensor[itk.D, Dimension]\n",
    "HessianImageType = itk.Image[HessianPixelType, Dimension]\n",
    "\n",
    "objectness_filter = itk.HessianToObjectnessMeasureImageFilter[\n",
    "    HessianImageType, ImageType\n",
    "].New()\n",
    "objectness_filter.SetBrightObject(True)\n",
    "objectness_filter.SetScaleObjectnessMeasure(False)\n",
    "objectness_filter.SetAlpha(0.5)\n",
    "objectness_filter.SetBeta(1.0)\n",
    "objectness_filter.SetGamma(5.0)\n",
    "objectness_filter.SetObjectDimension(1)\n",
    "\n",
    "multi_scale_filter = itk.MultiScaleHessianBasedMeasureImageFilter[\n",
    "    ImageType, HessianImageType, ImageType\n",
    "].New()\n",
    "multi_scale_filter.SetInput(input_image)\n",
    "multi_scale_filter.SetHessianToMeasureFilter(objectness_filter)\n",
    "multi_scale_filter.SetSigmaStepMethodToLogarithmic()\n",
    "multi_scale_filter.SetSigmaMinimum(sigma_minimum)\n",
    "multi_scale_filter.SetSigmaMaximum(sigma_maximum)\n",
    "multi_scale_filter.SetNumberOfSigmaSteps(number_of_sigma_steps)\n",
    "\n",
    "OutputPixelType = itk.UC\n",
    "OutputImageType = itk.Image[OutputPixelType, Dimension]\n",
    "\n",
    "rescale_filter = itk.RescaleIntensityImageFilter[ImageType, OutputImageType].New()\n",
    "rescale_filter.SetInput(multi_scale_filter)\n",
    "\n",
    "itk.imwrite(rescale_filter.GetOutput(), output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b27258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the tiff sequence obtained above and choose certain slices for annotation\n",
    "#perform annotations using napari\n",
    "#save the labels seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866448e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/images/1683_0596.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vesselness = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/itk_vesselness/1683_0596.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/masks-single channel/1683_0596.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_images = [\n",
    "    raw_image,\n",
    "    #gaussian(raw_image, sigma=1),\n",
    "    #gaussian(raw_image, sigma=5),\n",
    "    #sobel(gaussian(raw_image, sigma=1)),\n",
    "    #sobel(gaussian(raw_image, sigma=5)),\n",
    "    #vesselness,\n",
    "]\n",
    "tifffile.imsave('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/features_train/1683_0596.tif',feature_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/features_train/'\n",
    "os.remove('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/features_train/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c947db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainning_classifier = \"PixelClassifier.cl\"\n",
    "apoc.erase_classifier(Trainning_classifier)\n",
    "segmenter = apoc.PixelClassifier(opencl_filename=Trainning_classifier, max_depth = 10, num_ensembles = 50)\n",
    "\n",
    "# setup feature set used for training\n",
    "features = \"original\"\n",
    "\n",
    "# train classifier on folders\n",
    "apoc.train_classifier_from_image_folders(\n",
    "    segmenter, \n",
    "    features, \n",
    "    image = features_train, \n",
    "    ground_truth = masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d651b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_raw_tiff/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e227e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = sorted(os.listdir('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_raw_tiff/'))\n",
    "for filename in filename_list:\n",
    "    raw_image = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_raw_tiff/' + filename)\n",
    "    vesselness = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/vesselness/' + filename)\n",
    "    \n",
    "    feature_images = [raw_image, vesselness]\n",
    "        \n",
    "    tifffile.imwrite('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_vesselness_test/' + filename, feature_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83320f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_vesselness_test/'\n",
    "os.remove('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_vesselness_test/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_vesselness_test/'\n",
    "\n",
    "infiles = os.listdir(input_dir)\n",
    "\n",
    "for filename in infiles:\n",
    "    filepath = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_vesselness_test/' + filename)\n",
    "\n",
    "        \n",
    "    result = PixelClassifier(opencl_filename='PixelClassifier.cl').predict(image = filepath)\n",
    "    \n",
    "    tifffile.imwrite('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/result/' + filename, result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the binarization is done and the next step is skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/multilabel/vesselness/test/1683_binary_result.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc40c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_arr = np.array(binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_data_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72958e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = measure.label(binary_data_arr, background=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imsave('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/skeletonization/Labels.tif', Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = imread('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/skeletonization/Labels.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = Labels.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eed18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = np.transpose(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "skels = kimimaro.skeletonize(\n",
    "  Labels, \n",
    "  teasar_params={\n",
    "    'scale': 1.1,\n",
    "    'const': 50, # set scale = 1.1 and const = 10 for the data with blood vessels\n",
    "    'pdrf_exponent': 4,\n",
    "    'pdrf_scale': 100000,\n",
    "    'soma_detection_threshold': 10, # physical units\n",
    "    'soma_acceptance_threshold': 10, # physical units\n",
    "    'soma_invalidation_scale': 1.0,\n",
    "    'soma_invalidation_const': 10, # physical units\n",
    "    'max_paths': None, # default None\n",
    "  },\n",
    "  # object_ids=[ ... ], # process only the specified labels\n",
    "  # extra_targets_before=[ (27,33,100), (44,45,46) ], # target points in voxels\n",
    "  # extra_targets_after=[ (27,33,100), (44,45,46) ], # target points in voxels\n",
    "  dust_threshold=0, # skip connected components with fewer than this many voxels\n",
    "  anisotropy=(1,1,1), # default True\n",
    "  fix_branching=True, # default True\n",
    "  fix_borders=True, # default True\n",
    "  #fill_holes=False, # default False\n",
    "  #fix_avocados=False, # default False\n",
    "  progress=True, # default False, show progress bar\n",
    "  parallel=1, # <= 0 all cpu, 1 single process, 2+ multiprocess\n",
    "  parallel_chunk_size=50, # how many skeletons to process before updating progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f370ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Length : %d\" % len (skels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998686d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_folder = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/skeletonization/skeletons_scale1.1Const50/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570479ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = 'skeleton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(skels.items()),columns = ['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ab148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a89b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_to_swc(skel, outfn):\n",
    "    with open(outfn, \"w\") as f:\n",
    "        f.write(skel.to_swc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca154240",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    if label == 0:\n",
    "        continue\n",
    "    skel = skels[label]\n",
    "    skel_swc_fn = os.path.join(skeleton_folder, sample_name + \"_%i.swc\" % label)\n",
    "    skeleton_to_swc(skel, skel_swc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9121b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity check visualize the skeletons using: \"https://neuroinformatics.nl/HBP/morphology-viewer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5816e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for further analysis and visulaization of the skeletons pplotly was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5102e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##resampling of the skeleton files were done using the R package \"nat\" \n",
    "##run the following commands in R console\n",
    "\n",
    "\n",
    "# rgl.useNULL = TRUE\n",
    "\n",
    "# library(nat)\n",
    "\n",
    "\n",
    "\n",
    "# em_folder <- \"Path/to/the/skeletons/folder/\"\n",
    "# result_folder <- \"path/to/the/results/folder/\"\n",
    "# stepsize <- 10\n",
    "\n",
    "## read em neurons\n",
    "#ems = read.neurons(em_folder)\n",
    "\n",
    "##resample:\n",
    "\n",
    "#ems_rsmp = lapply(ems,nat::resample,stepsize=stepsize)\n",
    "\n",
    "#write.neurons(ems_rsmp,result_folder,format='swc',files=names(ems_rsmp))\n",
    "\n",
    "# warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging swcs of the resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c43b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input directory\n",
    "swc_dir = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/MST_ILP/merge_skeletons/results_skeletons_updated/\"\n",
    "#print(sorted(os.listdir(swc_dir)))\n",
    "# get list of SWC files\n",
    "swc_files = [f for f in sorted(os.listdir(swc_dir)) if f.endswith('.swc')]\n",
    "\n",
    "# initialize new SWC file with node ID offset\n",
    "new_swc = ''\n",
    "last_node_id = 0\n",
    "node_id_to_add = 0\n",
    "\n",
    "\n",
    "# iterate through SWC files and merge contents\n",
    "for f in swc_files:\n",
    "    \n",
    "    with open(os.path.join(swc_dir, f), 'r') as f:\n",
    "        swc_contents = f.read()\n",
    "        \n",
    "    lines = swc_contents.split('\\n')\n",
    "\n",
    "    lines.sort(key=lambda x: int(x.split()[0]) if not x.startswith('#') and x.strip() else float('inf'))\n",
    "        \n",
    "        \n",
    "    # iterate through lines in swc\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        \n",
    "        columns = line.split()\n",
    "        \n",
    "        if len(columns) < 7:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        node_id = int(columns[0]) + node_id_to_add\n",
    "        parent_id = int(columns[6])\n",
    "        if parent_id == -1:\n",
    "            new_parent_id = -1\n",
    "        else:\n",
    "            new_parent_id = parent_id + node_id_to_add\n",
    "        \n",
    "        \n",
    "        columns[0] = str(node_id)\n",
    "        columns[6] = str(new_parent_id)\n",
    "        \n",
    "        \n",
    "        new_line = ' '.join(columns)\n",
    "        new_swc += new_line + '\\n'\n",
    "        \n",
    "        last_node_id = int(columns[0])\n",
    "  \n",
    "    \n",
    "    node_id_to_add = last_node_id \n",
    "    \n",
    "# write new SWC file\n",
    "with open(os.path.join(swc_dir, '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/MST_ILP/merge_skeletons/skeletons_merged_resampled_updated.swc'), 'w') as f:\n",
    "    f.write(new_swc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of merged data with black background in the plot area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def read_swc(file_path):\n",
    "    nodes = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            if len(columns) != 7:\n",
    "                print(f\"Error: line {i+1} in file {file_path} has {len(columns)} columns instead of 7\")\n",
    "                continue\n",
    "            node_id = int(columns[0])\n",
    "            node_type = int(columns[1])\n",
    "            x, y, z = map(float, columns[2:5])\n",
    "            radius = float(columns[5])\n",
    "            parent_id = int(columns[6])\n",
    "            nodes.append((node_id, node_type, x, y, z, radius, parent_id))\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def plot_skeleton(nodes):\n",
    "     node_x, node_y, node_z = [], [], []\n",
    "    for node in nodes:\n",
    "        node_x.append(node[2])\n",
    "        node_y.append(node[3])\n",
    "        node_z.append(node[4])\n",
    "\n",
    "    # Create trace for nodes\n",
    "    node_trace = go.Scatter3d(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        z=node_z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=0.1,\n",
    "            color='red'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create trace for edges\n",
    "    edge_x, edge_y, edge_z = [], [], []\n",
    "    for node in nodes:\n",
    "        parent_id = node[6]\n",
    "        if parent_id > 0:\n",
    "            parent = next((n for n in nodes if n[0] == parent_id), None)\n",
    "            x, y, z = node[2], node[3], node[4]\n",
    "            edge_x += [parent[2], x, None]\n",
    "            edge_y += [parent[3], y, None]\n",
    "            edge_z += [parent[4], z, None]\n",
    "\n",
    "    edge_trace = go.Scatter3d(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        z=edge_z,\n",
    "        mode='lines',\n",
    "        line=dict(color='white', width=1),\n",
    "        hoverinfo='none'\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(\n",
    "        data=[edge_trace, node_trace],\n",
    "        layout=go.Layout(\n",
    "            title='Skeleton Plot',\n",
    "            plot_bgcolor='black',\n",
    "            scene=dict(\n",
    "                xaxis_title='X',\n",
    "                yaxis_title='Y',\n",
    "                zaxis_title='Z',\n",
    "                xaxis=dict(showgrid = False),  # Hide x-axis grid and zeroline\n",
    "                yaxis=dict(showgrid = False),\n",
    "                zaxis=dict(showgrid = False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(template='plotly_dark',\n",
    "                                plot_bgcolor='rgba(0, 0, 0, 0)',\n",
    "                                paper_bgcolor='rgba(0, 0, 0, 0)',) \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    swc_file_path = '/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/MST_ILP/merge_skeletons/skeletons_merged_resampled_updated.swc'\n",
    "    nodes = read_swc(swc_file_path)\n",
    "    plot_skeleton(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum probable path (by connecting the broken pieces of the blood vessel - different skeleton files using ) - refer connect_skeletons.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af74a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interskeleton connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe287b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage\n",
    "\n",
    "read skeletons\n",
    "skeleton_folder = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/MST_ILP/merge_skeletons/results_skeletons_updated/\"\n",
    "\n",
    "#\n",
    "skeletons = []\n",
    "for filename in os.listdir(skeleton_folder):\n",
    "    if filename.endswith(\".swc\"):\n",
    "        skeleton = []\n",
    "        with open(os.path.join(skeleton_folder, filename), \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                fields = line.strip().split()\n",
    "                if len(fields) < 7:\n",
    "                    continue\n",
    "                index = int(fields[0])\n",
    "                type_id = int(fields[1])\n",
    "                x = float(fields[2])\n",
    "                y = float(fields[3])\n",
    "                z = float(fields[4])\n",
    "                radius = float(fields[5])\n",
    "                parent = int(fields[6])\n",
    "                skeleton.append((x, y, z, index, parent))\n",
    "        skeletons.append(skeleton)\n",
    "\n",
    "#read raw data(image evidence)\n",
    "image = np.load(\"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/kristin_data/1683/1683_raw_tiff_(red).npy\")\n",
    "image = np.transpose(image)\n",
    "print(image.shape)\n",
    "\n",
    "normalized_image = 255 - image\n",
    "\n",
    "\n",
    "\n",
    "edges = []\n",
    "\n",
    "\n",
    "# for skeleton in skeletons:\n",
    "#     for i, point in enumerate(skeleton):\n",
    "        \n",
    "\n",
    "# Add edges based on image evidence\n",
    "threshold = 8000\n",
    "visited_skeletons = []\n",
    "for i, skeleton_i in enumerate(skeletons):\n",
    "    visited_skeletons.append(i)\n",
    "    #G.add_node(i, skeleton.index)\n",
    "    for j, skeleton_j in enumerate(skeletons[i+1:], start=i+1):\n",
    "        #G.add_node(j, skeleton.index)\n",
    "        for p in skeleton_i:\n",
    "            #print(p)\n",
    "            for q in skeleton_j:\n",
    "                \n",
    "                x1, y1, z1 = p[:3]\n",
    "                x2, y2, z2 = q[:3]\n",
    "                \n",
    "#                 print(x1, y1, z1)\n",
    "#                 print(x2, y2, z2)\n",
    "#                 z1 = z1 % image.shape[2]\n",
    "#                 z2 = z2 % image.shape[2]\n",
    "#                 G.add_node((p[3], i))\n",
    "#                 G.add_node((q[3], j))\n",
    "\n",
    "                # distance formula\n",
    "                distance = np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2)\n",
    "                \n",
    "                #print(x1, y1, z1, x2, y2, z2)\n",
    "                if distance <= 40 and (0 <= int(x1) <= normalized_image.shape[0] and \n",
    "                    0 <= int(y1) <= normalized_image.shape[1] and \n",
    "                    0 <= int(z1) <= normalized_image.shape[2] and \n",
    "                    0 <= int(x2) <= normalized_image.shape[0] and \n",
    "                    0 <= int(y2) <= normalized_image.shape[1] and \n",
    "                    0 <= int(z2) <= normalized_image.shape[2]):\n",
    "\n",
    "                    print('node found')\n",
    "                    start = [int(x1), int(y1), int(z1)]\n",
    "                    end = [int(x2), int(y2), int(z2)]\n",
    "                    print(p, q)\n",
    "\n",
    "                    # set bounding box between \n",
    "                    bbox = np.array([start, end])\n",
    "                    min_coords = np.min(bbox, axis=0)\n",
    "                    max_coords = np.max(bbox, axis=0)\n",
    "\n",
    "\n",
    "                    cropped_image = normalized_image[min_coords[0]:max_coords[0]+1,\n",
    "                                        min_coords[1]:max_coords[1]+1,\n",
    "                                        min_coords[2]:max_coords[2]+1]\n",
    "\n",
    "                    if (0 <= start[0]-min_coords[0] < cropped_image.shape[0] and\n",
    "                        0 <= start[1]-min_coords[1] < cropped_image.shape[1] and\n",
    "                        0 <= start[2]-min_coords[2] < cropped_image.shape[2] and\n",
    "                        0 <= end[0]-min_coords[0] < cropped_image.shape[0] and\n",
    "                        0 <= end[1]-min_coords[1] < cropped_image.shape[1] and\n",
    "                        0 <= end[2]-min_coords[2] < cropped_image.shape[2]):\n",
    "\n",
    "\n",
    "                        path = skimage.graph.route_through_array(\n",
    "                                       cropped_image,\n",
    "                                       [start[0]-min_coords[0], start[1]-min_coords[1], start[2]-min_coords[2]],\n",
    "                                       [end[0]-min_coords[0], end[1]-min_coords[1], end[2]-min_coords[2]],\n",
    "                                       fully_connected=True,\n",
    "                                       geometric=True,\n",
    "                                             )\n",
    "                        if path[1] < threshold:\n",
    "                            #G.add_edge(p, q)\n",
    "                            edge = (p, q, path[1])\n",
    "                            edges.append(edge)\n",
    "                            print('edge successfully added')\n",
    "                        else:\n",
    "                            print(\"skipping the incorrect edge\")\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                else:\n",
    "#                     print(f\"skipping point {j} in {skeleton_j} as it is too far from point {i} in {skeleton_j}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "df = pd.DataFrame(edges, columns=['node1', 'node2', 'intensity'])\n",
    "\n",
    "# Save DataFrame as CSV\n",
    "df.to_csv('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/MST_ILP/merge_skeletons/additional_edges_8k_40_(transpose).csv', index=False)\n",
    "print(\"possible edges that could connect broken skeletons are determined\")\n",
    "\n",
    "num_skeletons = len(skeletons)\n",
    "if len(visited_skeletons) == num_skeletons:\n",
    "    print(\"All skeletons have been visited.\")\n",
    "else:\n",
    "    print(\"Some skeletons have not been visited.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0712b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intraskeleton_connections -- only based on the distance between the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_folder = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/intraskeleton_connections/skeletons/\"\n",
    "\n",
    "skeletons = []\n",
    "\n",
    "for filename in os.listdir(skeleton_folder):\n",
    "    if filename.endswith(\".swc\"):\n",
    "        skeleton = []\n",
    "        with open(os.path.join(skeleton_folder, filename), \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                fields = line.strip().split()\n",
    "                if len(fields) < 7:\n",
    "                    continue\n",
    "                index = int(fields[0])\n",
    "                type_id = int(fields[1])\n",
    "                x = int(round(float(fields[2])))\n",
    "                y = int(round(float(fields[3])))\n",
    "                z = int(round(float(fields[4])))\n",
    "                radius = float(fields[5])\n",
    "                parent = int(fields[6])\n",
    "                skeleton.append((x, y, z, radius, parent))\n",
    "        skeletons.append(skeleton)\n",
    "        \n",
    "print(len(skeletons))\n",
    "\n",
    "additional_edges = []\n",
    "def euclidean_distance(p1, p2):\n",
    "    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)\n",
    "\n",
    "\n",
    "for i, skeleton_i in enumerate(skeletons):\n",
    "    for p in skeleton_i:\n",
    "        x1, y1, z1,radius1 = p[:4]\n",
    "        for q in skeleton_i:\n",
    "            if p == q:\n",
    "                continue\n",
    "            x2, y2, z2, radius2 = q[:4]\n",
    "            distance = euclidean_distance((x1, y1, z1), (x2, y2, z2))\n",
    "            radius_difference = abs(radius1 - radius2)\n",
    "            if radius1 < 20 and radius2 < 20 and distance < 20 and radius_difference < 5:\n",
    "                additional_edges.append(((x1,y1,z1), (x2,y2,z2)))\n",
    "print(len(additional_edges))\n",
    "print(additional_edges[2])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(additional_edges, columns=['node1', 'node2'])\n",
    "\n",
    "\n",
    "df.to_csv('/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/intraskeleton_connections/additional_edges_intraskeleton.csv', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ef53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find root nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84992a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert multipage tiff to nii to view in ITKSNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiff_to_nifti(input_tiff_path, output_nifti_path):\n",
    "    #input\n",
    "    tiff_image = tiff.imread(input_tiff_path)\n",
    "    tiff_array = np.array(tiff_image)\n",
    "    sitk_image = sitk.GetImageFromArray(tiff_array)\n",
    "    sitk_image.SetOrigin((0, 0, 0))\n",
    "    sitk_image.SetSpacing((1, 1, 1))\n",
    "    #output\n",
    "    sitk.WriteImage(sitk_image, output_nifti_path)\n",
    "\n",
    "input_tiff_path = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/ITKsnap/Label_1_16bit.tif\"\n",
    "output_nifti_path = \"/Users/ramyarajalakshmi/Documents/Segmentation/mv_heart/ITKsnap/Label_1_16bit.nii\"\n",
    "tiff_to_nifti(input_tiff_path, output_nifti_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71940a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference point --> manually selected \n",
    "x1, y1, z1 = (263.0, 951.0, 1101.0)\n",
    "\n",
    "possible_roots_CS = []\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)\n",
    "\n",
    "for node in nodes:\n",
    "    x2, y2, z2, radius = node\n",
    "    \n",
    "    distance = euclidean_distance((x1, y1, z1), (x2, y2, z2))\n",
    "    \n",
    "    if distance <= 50: #-->distance threshold\n",
    "        possible_roots_CS.append(node)\n",
    "\n",
    "if len(possible_roots_CS) > 0:\n",
    "    # Find the possible root with the largest radius\n",
    "    largest_radius_root = max(possible_roots_CS, key=lambda x: x[3])\n",
    "    print(\"Possible Roots within 30 units:\")\n",
    "    for root in possible_roots_CS:\n",
    "        print(root)\n",
    "    print(\"\\nPossible Root with Largest Radius:\")\n",
    "    print(largest_radius_root)\n",
    "else:\n",
    "    print(\"No possible roots found within 30 units.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf47e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roots (roots_count = len(roots)) veins(72.0, 500.0, 816.0), artery1((125.269491197135, 534.730508802865, 1096.73050880286, 30.1874740496187))\n",
    "# artery2 (117.140509530041, 822.140509530041, 823.140509530041, 18.2015584800992)\n",
    "#(104.324012949113, 793.324012949113, 854.324012949113, 15.5912973888118)\n",
    "# artery3 (252.926940638227, 936.926940638227, 1094.92694063823, 10.630146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c55b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step -- create overconnected graph with edges from skeletonization, additional edges from interskeleton and intraskeleton connectoions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5691d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract largest connected component and dump the smaller components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ILP formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ILP input -- nodes, edges and the root nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce441c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a Gurobi model to extract four trees\n",
    "model = Model(\"OptimalTrees\")\n",
    "\n",
    "\n",
    "V = result_nodes  \n",
    "E =  edges\n",
    "roots = ((72.0, 500.0, 816.0, 16.25), # vein -- coronary sinus confirmed with Ralf\n",
    "         (125.269491197135, 534.730508802865, 1096.73050880286, 30.1874740496187), #aretry 1\n",
    "         (117.140509530041, 822.140509530041, 823.140509530041, 18.2015584800992), #artery2\n",
    "         (252.926940638227, 936.926940638227, 1094.92694063823, 10.630146)) # artery3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decision variables -- prone to changes depending upon the constraints and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783564a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Binary variables for each edge for edge selection\n",
    "edges_selected = {}  \n",
    "for edge in E:\n",
    "    edges_selected[edge] = model.addVar(vtype=GRB.BINARY, name=f\"edge_{edge}\")\n",
    "\n",
    "# Binary variables for assigning nodes to trees depending upon the root they are rooted to\n",
    "nodes_in_tree = {}  # A nested dictionary to store binary variables for nodes and roots\n",
    "for root in roots:\n",
    "    nodes_in_tree[root] = {}\n",
    "    for node in V:\n",
    "        nodes_in_tree[root][node] = model.addVar(vtype=GRB.BINARY, name=f\"root_{root}_node_{node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraints --geometrical priors from skeletonized data --node pairs selected as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# width_consistency - first extrac eligible nodes and the width differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_nodes = []\n",
    "width_differences = []\n",
    "node_dict = {node[0]: node for node in nodes}\n",
    "\n",
    "for node in nodes:\n",
    "    node_id, x, y, z, radius, parent = node\n",
    "    \n",
    "    #larger nodes are not selected bcz it also includes the weird chamber region which might affect the statistics\n",
    "    if parent != -1 and radius < 30:\n",
    "        if parent in node_dict and node_dict[parent][4] > radius:\n",
    "\n",
    "            width_difference = node_dict[parent][4] - radius\n",
    "            width_differences.append(width_difference)  \n",
    "\n",
    "            current_node_set = set(node)\n",
    "            parent_node_set = set(node_dict[parent])\n",
    "            \n",
    "            if current_node_set not in map(set, eligible_nodes):\n",
    "                eligible_nodes.append(node)\n",
    "            if parent_node_set not in map(set, eligible_nodes):\n",
    "                eligible_nodes.append(node_dict[parent])\n",
    "\n",
    "print(len(eligible_nodes))\n",
    "print(eligible_nodes[2])\n",
    "print(len(width_differences))\n",
    "#print(width_differences[50])\n",
    "#print(width_differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77854f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(width_differences, bins=180, density=True, alpha=0.6, color='blue')\n",
    "\n",
    "plt.xlabel('Width Differences')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Width Differences Histogram')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fit this to a suitable curve and find the mean\n",
    "# assign cost to the edges based on the width differences\n",
    "# similarly assign costs to the edges based on other geometric priors like the orientation consistency, tortuosity, branching angle\n",
    "# find sum of the cost for each edge\n",
    "#construct objective function in such a way that the objective function optimizes for the minimum cost trees\n",
    "#add other constraints like 4 trees, no loops, width decreases from root to end nodes, --more like global constraints\n",
    "#use horton's law, murray's law, bifurcation quantity for post processing verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176a567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari-env",
   "language": "python",
   "name": "napari-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
